{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07a9b892",
   "metadata": {},
   "source": [
    "# Coinbase Double DQN Training Pipeline\n",
    "\n",
    "This notebook implements a Deep Q-Network (DQN) with enhancements such as:\n",
    "- **Prioritized Experience Replay (PER)**\n",
    "- **Batch Normalization**\n",
    "- **Live market data integration via WebSockets**\n",
    "- **Signal handling for graceful shutdown**\n",
    "\n",
    "The goal is to train a robust reinforcement learning agent to identify profitable trades in real-time USDC-crypto markets using Coinbase data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0423e263",
   "metadata": {},
   "source": [
    "# Coinbase Dueling DQN Agent\n",
    "A professional training notebook using real-time USDC price data from Coinbase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ec4f1a",
   "metadata": {},
   "source": [
    "## Initialize Signal Handling\n",
    "Gracefully manage interruptions and shutdowns during training or data streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f7c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import signal\n",
    "import time\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Constants\n",
    "GAMMA = 0.99\n",
    "LR = 1e-3\n",
    "EPSILON = 1.0\n",
    "EPSILON_DECAY = 0.995\n",
    "MIN_EPSILON = 0.01\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_CAPACITY = 10000\n",
    "TARGET_UPDATE = 10\n",
    "CACHE_FILE = \"coinbase_cache.json\"\n",
    "API_URL = \"https://api.coinbase.com/v2/prices/USDC-USD/spot\"\n",
    "SAVE_MODEL_FREQ = 50\n",
    "SAVE_BEST_MODEL = True\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"coinbase_dqn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5225b09",
   "metadata": {},
   "source": [
    "## Define Replay Buffer with Prioritized Experience Replay (PER)\n",
    "Efficient sampling mechanism that prioritizes high-error transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5099902",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.value_fc = nn.Linear(128, 1)\n",
    "        self.advantage_fc = nn.Linear(128, 2)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        value = self.value_fc(x)\n",
    "        advantage = self.advantage_fc(x)\n",
    "        return value + advantage - advantage.mean()\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6, beta=0.4):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.priorities = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, td_error):\n",
    "        self.buffer.append((state, action, reward, next_state, td_error))\n",
    "        self.priorities.append(max(td_error, 1e-6))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return []\n",
    "        priorities = np.array(self.priorities) ** self.alpha\n",
    "        probs = priorities / priorities.sum()\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        batch = [self.buffer[idx] for idx in indices]\n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-self.beta)\n",
    "        weights /= weights.max()\n",
    "        return batch, weights, indices\n",
    "\n",
    "    def update_priorities(self, indices, td_errors):\n",
    "        for idx, td_error in zip(indices, td_errors):\n",
    "            self.priorities[idx] = max(td_error, 1e-6)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9537281d",
   "metadata": {},
   "source": [
    "## Build the DQN Model\n",
    "Includes convolutional layers with Batch Normalization for improved training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb30ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "policy_net = DuelingDQN().to(device)\n",
    "target_net = DuelingDQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "buffer = PrioritizedReplayBuffer(BUFFER_CAPACITY)\n",
    "step_count = 0\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "shutdown_event = threading.Event()\n",
    "\n",
    "def fetch_data():\n",
    "    try:\n",
    "        response = requests.get(API_URL, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        return float(data.get('data', {}).get('amount', 0))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Data fetch failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_cache():\n",
    "    try:\n",
    "        with open(CACHE_FILE, 'w') as f:\n",
    "            json.dump([exp for exp in buffer.buffer], f)\n",
    "        logger.info(f\"Cache saved to {CACHE_FILE}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save cache: {e}\")\n",
    "\n",
    "def save_best_model():\n",
    "    global best_model, best_loss\n",
    "    current_loss = calculate_loss(torch.zeros(1, 4).to(device), [0], [0], torch.zeros(1, 4).to(device))\n",
    "    if current_loss < best_loss:\n",
    "        best_loss = current_loss\n",
    "        best_model = policy_net.state_dict()\n",
    "        torch.save(best_model, \"best_dqn_model.pth\")\n",
    "        logger.info(\"New best model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984a472",
   "metadata": {},
   "source": [
    "## Configure Optimizer and Loss\n",
    "Set up the optimizer and loss function for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353926a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(states, actions, rewards, next_states):\n",
    "    states = torch.FloatTensor(np.array(states)).to(device)\n",
    "    next_states = torch.FloatTensor(np.array(next_states)).to(device)\n",
    "    actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "\n",
    "    q_values = policy_net(states).gather(1, actions)\n",
    "    q_next = target_net(next_states).max(1)[0].unsqueeze(1).detach()\n",
    "    target = rewards + (GAMMA * q_next)\n",
    "\n",
    "    loss = F.mse_loss(q_values, target)\n",
    "    return loss\n",
    "\n",
    "def compute_td_error(state, action, reward, next_state):\n",
    "    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "    next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "    q_value = policy_net(state_tensor)[0, action]\n",
    "    with torch.no_grad():\n",
    "        q_next = target_net(next_state_tensor).max(1)[0].detach()\n",
    "    return reward + GAMMA * q_next.item() - q_value.item()\n",
    "\n",
    "def select_action(state):\n",
    "    global EPSILON\n",
    "    if np.random.rand() < EPSILON:\n",
    "        return np.random.choice([0, 1])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            return policy_net(state_tensor).argmax().item()\n",
    "\n",
    "def update_model(batch, weights):\n",
    "    states, actions, rewards, next_states, _ = zip(*batch)\n",
    "    loss = calculate_loss(states, actions, rewards, next_states)\n",
    "    weights_tensor = torch.FloatTensor(weights).to(device)\n",
    "    loss = (loss * weights_tensor).mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def update_epsilon(step_count):\n",
    "    global EPSILON\n",
    "    if EPSILON > MIN_EPSILON:\n",
    "        EPSILON *= EPSILON_DECAY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e44107",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "Manages environment interaction, model updates, and experience replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05f81a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_history = []\n",
    "\n",
    "def train_loop():\n",
    "    global step_count\n",
    "    for episode in range(1, 101):\n",
    "        state = np.random.rand(4)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = select_action(state)\n",
    "            next_price = fetch_data()\n",
    "            if next_price is None:\n",
    "                continue\n",
    "            next_state = np.array([next_price, state[1], state[2], state[3]])\n",
    "            reward = 0.1 if action == 0 else -0.1\n",
    "            total_reward += reward\n",
    "\n",
    "            td_error = compute_td_error(state, action, reward, next_state)\n",
    "            buffer.push(state, action, reward, next_state, td_error)\n",
    "\n",
    "            if len(buffer) >= BATCH_SIZE:\n",
    "                batch, weights, indices = buffer.sample(BATCH_SIZE)\n",
    "                update_model(batch, weights)\n",
    "\n",
    "            state = next_state\n",
    "            step_count += 1\n",
    "            update_epsilon(step_count)\n",
    "\n",
    "            if step_count % TARGET_UPDATE == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "            if step_count % SAVE_MODEL_FREQ == 0:\n",
    "                torch.save(policy_net.state_dict(), f\"dqn_model_{step_count}.pth\")\n",
    "            if SAVE_BEST_MODEL:\n",
    "                save_best_model()\n",
    "\n",
    "            done = np.random.rand() < 0.01\n",
    "        reward_history.append(total_reward)\n",
    "    logger.info(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd33e11",
   "metadata": {},
   "source": [
    "## Run Main Training Routine\n",
    "Execute the training loop, stream live data, and ensure graceful exits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756f45a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(reward_history)\n",
    "plt.title('Total Rewards per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235b5d60",
   "metadata": {},
   "source": [
    "## Run Main Training Routine\n",
    "Execute the training loop, stream live data, and ensure graceful exits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f2b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
